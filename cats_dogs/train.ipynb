{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "from collection import s3_download_file\n",
    "from preprocessing import preprocess\n",
    "from prediction import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an unzip the dataset\n",
    "S3_DATA_FILE = os.getenv(\"s3_data_file\", \"kagglecatsanddogs_5340.zip\")\n",
    "DOWNLOAD_PATH = Path(\".cache/data.zip\")\n",
    "\n",
    "if not DOWNLOAD_PATH.is_file():\n",
    "    s3_download_file(S3_DATA_FILE, DOWNLOAD_PATH)\n",
    "    !unzip -n -q .cache/data.zip -d .cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 1590 corrupted images.\n",
      "Classes: ['Cat', 'Dog']\n",
      "Features first batch size: torch.Size([32, 3, 50, 50])\n",
      "Labels first batch size: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Directory path of your dataset\n",
    "data_dir = '.cache/PetImages'\n",
    "\n",
    "# Preprocess data: clean, resize,\n",
    "# split into test and validation subsets...\n",
    "train_loader, val_loader, _, dataset = preprocess(data_dir)\n",
    "\n",
    "# Verify the class labels of the dataset\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Verify sizes\n",
    "for images, labels in train_loader:\n",
    "    print(\"Features first batch size:\", images.size())\n",
    "    print(\"Labels first batch size:\", labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns a tuple of two batches (images and labels).\n",
    "The images batch contains 32 images.\n",
    "The labels batch contains the corresponding 32 labels for those images.\n",
    "\n",
    "Each image has 50x50 pixels, with 3 color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /opt/app-root/src/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 154MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n"
     ]
    }
   ],
   "source": [
    "# Define the model (use a pre-trained ResNet and modify the final layer)\n",
    "model = models.resnet18(weights=\"DEFAULT\")\n",
    "num_features = model.fc.in_features\n",
    "# Modify final layer to define 2 output classes: Cat and Dog\n",
    "model.fc = nn.Linear(num_features, len(dataset.classes))\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: \n",
      " Training Loss: 0.4244\n",
      " Validation Loss: 0.3762, Accuracy: 0.8183\n",
      "Epoch 2/3: \n",
      " Training Loss: 0.2900\n",
      " Validation Loss: 0.2926, Accuracy: 0.8673\n",
      "Epoch 3/3: \n",
      " Training Loss: 0.2464\n",
      " Validation Loss: 0.3950, Accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: \\n Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct.double() / len(val_loader.dataset)\n",
    "    print(f' Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test to validate that the model works\n",
    "result = predict(\"test_cat.jpg\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a random Torch to specify\n",
    "# the inputs dimensions expected by the model\n",
    "first_batch_example = torch.randn(1, 3, 50, 50).to(device)\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    first_batch_example,\n",
    "    \"model.onnx\",\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
